---
###################################################
# Name: gds_aws_to_s3 role
#
# Description:  This role is intended to copy existing database(s) and user(s) from one RDS instance and store it in an s3 bucket.
#
# Parameters:  
#   Required:  endpoint=aws_source_endpoint
#              s3bucket=aws_s3_bucket_name
#   Optional:  preserve=yes {"src_dbs":["one","two","three"]}
#              user=aws_user password=aws_password 
#
#              Parms can also be passed via a json file as -e "@test.json"
#
#              Sample file:
#              endpoint: vis-stg.cluster-ccoxqscq6x7v.us-west-1.rds.amazonaws.com
#              s3bucket: my_bucket
#              src_dbs: ["vis_stg","test12"]
#
# Version:     1.0.0
# Date:        2021-01-27
#
# Notes:  
#         1. AWS user/pass order of operation.  
#            1) Provided on command line 
#            2) Optional .my.cnf in user's home directory on node (not the server the role is running on)
#            3) Default, encrypted values in role.
#         2. A decryption key MUST be provided for the role to run.  Add the key to ~/.vault on the server
#            that the role is running from.
#         3. Dump files generated mydumper are deleted, by default, when the load finishes.
#            Add preserve=yes to override this.
#
###################################################

- name: Get user names and passwords
  block:
    - name: Does ~/.my.cnf exist?
      stat: 
        path: ~/.my.cnf
      register: my_cnf
  
    - name: Retrieve AWS user, if ~/.my.cnf exists
      shell: grep user ~/.my.cnf | cut -d= -f2
      register: info
      changed_when: false
      when: my_cnf.stat.exists == true

    - name: Set AWS user
      set_fact:
        user: "{{ info.stdout | default(def_aws_user)}}"

    - name: Retrieve AWS password, if ~/.my.cnf exists
      shell: grep password ~/.my.cnf | cut -d= -f2
      register: info
      changed_when: false
      when: my_cnf.stat.exists == true

    - name: Set AWS password
      set_fact:
        password: "{{ info.stdout | default(def_aws_pass)}}"
    
- name: Confirm connection to endpoint
  shell: mysql -h {{ endpoint }} -u {{ user }} -p'{{ password }}' --connect-timeout=5 -BAN -e "SELECT 1"
  changed_when: false
  register: select_cmd
  failed_when: select_cmd.rc != 0
  
- name: Set defaults
  set_fact:
    all_dbs: true
  when: src_dbs is not defined

- name: Do source database(s) exist?
  block:
    - name: Build db list
      set_fact:
        src_dbs_list: "{{ src_dbs| join(\"','\") }}"

    - name: Query endpoint
      shell: mysql -h {{ endpoint }} -u {{ user }} -p'{{ password }}' -BAN -e "SELECT schema_name FROM information_schema.SCHEMATA WHERE schema_name IN ('{{ src_dbs_list }}')"
      changed_when: false
      register: src_dbs_out

    - name: 
      set_fact:
        src_dbs_out_list: "{{ src_dbs_out.stdout_lines }}"

    - name: Source database(s) requested
      debug:
        msg: "{{ src_dbs }}"

    - name: Source database(s) on endpoint
      debug:
        msg: "{{ src_dbs_out_list }}"

    - name: Compare database lists
      fail: 
        msg: "Backup aborted, due to database(s) not existing on endpoint" 
      when: src_dbs | difference(src_dbs_out_list) | length | int > 0
  when: all_dbs is not defined

- name: Build list of source databases
  block:
    - name: Get list of databases to dump
      shell: mysql -h {{ endpoint }} -u {{ user }} -p'{{ password }}' -BAN -e "SELECT schema_name FROM information_schema.SCHEMATA WHERE schema_name NOT IN ('information_schema','mysql','performance_schema','sys','hydra','maatkit','percona','tmp')"
      changed_when: false
      register: src_db_raw
      
    - name: Build db list
      set_fact:
        src_dbs: "{{ src_db_raw.stdout_lines }}"
  
    - name: Source database(s) on endpoint
      debug:
        msg: "{{ src_dbs }}"
        
  when: all_dbs is defined
  
- name: Get source database sizes
  block:
    - name: Build db size list for query
      set_fact:
        size_list: "{{ src_dbs | join(\"','\") }}"

    - name: Get total size of databases
      shell: mysql -h {{ endpoint }} -u {{ user }} -p'{{ password }}' -BAN -e "SELECT ROUND(SUM(data_length + index_length) / 1024 / 1024 / 1024, 1) AS 'Size' FROM information_schema.tables WHERE table_schema IN ('{{ size_list }}')"
      changed_when: false
      register: db_size

    - name: Report source database sizes
      debug:
        msg: "Database list: '{{ size_list }}', size: {{ db_size.stdout }} GB"
        
- name: Set datetime and instance
  set_fact:
    dt: "{{ '%Y%m%d-%H%M%S' | strftime }}"
    instance: "{{ endpoint }}"

- name: Remove previous dump files, if present
  file:
    path: /var/groupon/data/dump/{{ instance }}-{{ dt }}
    state: absent
  become: true

- name: Create source backup location
  file:
    path: /var/groupon/data/dump/{{ instance }}-{{ dt }}
    owner: root
    group: gds
    mode: 0770
    state: directory
  become: true

- name: Dump users from endpoint
  shell: pt-show-grants --noheader -h {{ endpoint }}  -u {{ user }}  -p'{{ password }}' --ignore='rdsadmin@localhost' > /var/groupon/data/dump/{{ instance }}-{{ dt }}/src_users.sql

- name: Build mydumper db list
  set_fact:
    myd_db_list: "{{ src_dbs | join('|') }}"

- name: Dump database(s)
  shell: cd /var/groupon/data/dump/{{ instance }}-{{ dt }} && /usr/bin/mydumper --regex '^({{ myd_db_list }})' --statement-size {{ mydumper.statement_size }} --rows {{ mydumper.rows }} --compress --no-locks --logfile /tmp/{{ instance }}.log --threads {{ mydumper.threads }} --verbose 3 -h {{ endpoint }}  --outputdir /var/groupon/data/dump/{{ instance }}-{{ dt }} --triggers --routines --events --user {{ user }} --password '{{ password }}'
  
- name: Create a single file archive
  archive:
    path: /var/groupon/data/dump/{{ instance }}-{{ dt }}
    dest: /var/groupon/data/dump/{{ instance }}-{{ dt }}.tar
    format: tar
    
- name: Create S3 Bucket
  s3_bucket:
    name: "{{ s3bucket }}"
    state: present
    tags:
      service_tag: daas_mysql
      owner_tag: gds@groupon.com

- name: Configure a lifecycle rule to transition files to glacier after 30
  s3_lifecycle:
    name: "{{ s3bucket }}"
    state: present
    status: enabled
    storage_class: glacier
    transition_days: 30

- name: Upload backup to S3
  aws_s3:
    bucket: "{{ s3bucket }}"
    object: "/{{ instance }}/{{ dt }}.tar"
    src: "/var/groupon/data/dump/{{ instance }}-{{ dt }}.tar"
    mode: put

- name: Remove dump and view files 
  block:
    - name: Remove Dump Directory
      file: path=/var/groupon/data/dump/{{ instance }}-{{ dt }} state=absent
    - name: Remove Tar File
      file: path=/var/groupon/data/dump/{{ instance }}-{{ dt }}.tar state=absent
  become: true
  when: (preserve is not defined or (preserve|upper != "YES" or preserve == False))

